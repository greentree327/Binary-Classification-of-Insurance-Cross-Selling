{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "# feature engineering to integrate  column 'id' with index column\n",
    "df = pd.read_csv(\"train.csv\", index_col='id')\n",
    "df_test = pd.read_csv(\"test.csv\", index_col='id')\n",
    "df_sample_submission = pd.read_csv(\"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Check the structure of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.1 Check missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.2 Visualize first 5 rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.3 Check how many samples in each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "for i in np.unique(df[\"Response\"]):\n",
    "    print(\"There are %d samples in train data for category [%s].\" % (np.sum(df[\"Response\"] == i), str(i)))\n",
    "\n",
    "# class imbalance exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Make Categorical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "def preprocess(X):\n",
    "    # creates a copy of X to avoid altering the originial data\n",
    "    X = X.copy()\n",
    "    # We use label encoding \n",
    "    X['Gender'] = (X['Gender']=='Male').astype(np.uint8)\n",
    "\n",
    "    # We use boolean mask\n",
    "    X['Vehicle_Damage'] = (X['Vehicle_Damage'] == 'Yes').astype(np.uint8)\n",
    "\n",
    "    # We use label encoding\n",
    "    X[\"Vehicle_Age\"] = X[\"Vehicle_Age\"].astype('category').cat.rename_categories({\n",
    "            \"1-2 Year\": 1, \"< 1 Year\": 0, \"> 2 Years\": 2}).astype('int8')\n",
    "    X['Age'] = X['Age'].astype('int8')\n",
    "    X['Driving_License'] = X['Driving_License'].astype('int8')\n",
    "    X['Region_Code'] = X['Region_Code'].astype('int8')\n",
    "    X['Previously_Insured'] = X['Previously_Insured'].astype('int8')\n",
    "    X['Annual_Premium'] = X['Annual_Premium'].astype('int32')\n",
    "    X['Policy_Sales_Channel'] = X['Policy_Sales_Channel'].astype('int16')\n",
    "    X['Vintage'] = X['Vintage'].astype('int16')\n",
    "    return X\n",
    "X_preprocess = preprocess(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Box Plots (compare the distribution of a continuous feature across different classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plotting the boxplots in a 2x2 grid\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "sns.boxplot(x='Response', y='Age', data=df, ax=axs[0, 0])\n",
    "axs[0, 0].set_title('Box Plot of Age by Response')\n",
    "axs[0, 0].set_ylabel(\"Age\")\n",
    "\n",
    "sns.boxplot(x='Response', y='Annual_Premium', data=df, ax=axs[0, 1])\n",
    "axs[0, 1].set_title('Box Plot of Annual Premium by Response')\n",
    "axs[0, 1].set_ylabel(\"Annual_Premium\")\n",
    "\n",
    "sns.boxplot(x='Response', y='Policy_Sales_Channel', data=df, ax=axs[1, 0])\n",
    "axs[1, 0].set_title('Box Plot of Policy Sales Channel by Response')\n",
    "axs[1, 0].set_ylabel(\"Policy_Sales_Channel\")\n",
    "\n",
    "sns.boxplot(x='Response', y='Vintage', data=df, ax=axs[1, 1])\n",
    "axs[1, 1].set_title('Box Plot of Vintage by Response')\n",
    "axs[1, 1].set_ylabel(\"Vintage\")\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set_xlabel('Response')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Bar Plot( Show the count of each categorical data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['Gender', 'Driving_License', 'Region_Code', 'Previously_Insured', 'Vehicle_Age', 'Vehicle_Damage']\n",
    "\n",
    "# Set up the matplotlib figure and axis grid\n",
    "fig, axes = plt.subplots(3, 2, figsize=(14, 12))\n",
    "fig.suptitle('Bar Plots of Categorical Variables', fontsize=16)\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Iterate over the variables and create a bar plot for each\n",
    "for ax, var in zip(axes, variables):\n",
    "    sns.countplot(x=var, hue = \"Response\", data=X, ax=ax, palette='viridis')\n",
    "    ax.set_title(f'Bar Plot of {var}')\n",
    "    ax.set_xlabel(var)\n",
    "    ax.set_ylabel('Count')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust the rect to make room for the suptitle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Train, test, valid split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold, StratifiedKFold, train_test_split\n",
    "\n",
    "X, X_test = train_test_split(X_preprocess , test_size=100000, random_state = 31, stratify = X_preprocess['Response'])\n",
    "X_train, X_valid = train_test_split(X, test_size=100000, random_state=31, stratify=X['Response'])\n",
    "# pop('Response'): extract the 'Response' column and remove it from df\n",
    "y_train = X_train.pop('Response')\n",
    "y_valid = X_valid.pop('Response')\n",
    "# X_test = X_valid; y_test = y_valid\n",
    "y_test = X_test.pop('Response')\n",
    "# training set is used to train the model(largest)\n",
    "# Validation set is used to evaluate model performance, and tune the hyperparameters of the model\n",
    "# test set is used to provide an unbiased estimate of the model's performance on unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data structure conversion -- DMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp # what is sparse matrix?\n",
    "import xgboost as xgb # import entire xgboost library\n",
    "\n",
    "# DMatrix is a data structure optimized for XGBoost, can be use in both device : CPU/GPU\n",
    "def prepare_DMatrix(training_set, validation_data):\n",
    "    # convert training data to CuPy arrays\n",
    "    \n",
    "    training_set_gpu = cp.array(training_set)\n",
    "    validation_data_gpu = cp.array(validation_data)\n",
    "    \n",
    "    DMatrix = xgb.DMatrix(training_set_gpu, label = validation_data_gpu)\n",
    "\n",
    "    return DMatrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Downsampling of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def down_sampling(X, y, i):\n",
    "    majority_class = X[y == 0].copy() # Extract rows where Response = 0\n",
    "    minority_class = X[y == 1].copy() # Extract rows where Response = 1\n",
    "    sample_size = len(minority_class) # Size of the minority class\n",
    "    # Train_test_split is used to split the majority class into 2 parts\n",
    "    majority_sample, X_rest, y_sample, y_rest = train_test_split(majority_class, y[y == 0], \n",
    "                                                                  train_size=sample_size, random_state=i, \n",
    "                                                stratify=majority_class['Age'])\n",
    "    # train_size/test_size: If int, represents the absolute number of train samples.\n",
    "    # If float, (0.0,1.0), represent the proportion of the dataset to include in the split\n",
    "    # If None, automatically set to the complement of the test size\n",
    "\n",
    "    # majority sample is a subset of majority class, same size as minority class, stratified by age\n",
    "\n",
    "    # X_minimial combines \"majority sample\" and \"minority class\" to create a balanced subset\n",
    "    X_minimal = pd.concat([majority_sample, minority_class], axis=0)\n",
    "    # y_minimial combines the target variables of both \"majority sample\" and \"minority class\"\n",
    "    y_minimal = pd.concat([y_sample, y[y == 1]], axis=0)\n",
    "\n",
    "\n",
    "    return X_minimal, y_minimal, X_rest, y_rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_minimal, y_minimal, X_rest, y_rest = down_sampling(X_train, y_train, 63) # random_state is 63"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Performance of a downsampled dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we measure how does XGBoost performs in a downsampled, balanced dataset, compare to a random sampling dataset of the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc',\n",
    "    'max_bin': 262143,\n",
    "    'tree_method': 'hist',\n",
    "    'device' : 'cuda', \n",
    "    'seed' : 42\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_scores, test_scores2 = [], []\n",
    "\n",
    "dvalid = prepare_DMatrix(X_valid, y_valid)\n",
    "dtest = prepare_DMatrix(X_test, y_test)\n",
    "\n",
    "dminimal = prepare_DMatrix(X_minimal, y_minimal)\n",
    "# Training with early stopping\n",
    "bst = xgb.train(params, dminimal, num_boost_round=2000, evals=[(dvalid, 'validation')],\n",
    "                early_stopping_rounds=100, verbose_eval=False)\n",
    "\n",
    "# Predict probabilities for the test set\n",
    "y_pred_proba = bst.predict(dtest)\n",
    "\n",
    "# Evaluate and append ROC AUC score\n",
    "roc_auc_downsample = roc_auc_score(y_test, y_pred_proba)\n",
    "test_scores.append(roc_auc_downsample)\n",
    "print(f\"ROC AUC Downsample Score: {roc_auc_downsample}\")\n",
    "\n",
    "# clean up memory\n",
    "del bst, dminimal\n",
    "gc.collect()\n",
    "\n",
    "# random sampling \n",
    "X_t2, _, y_t2, _ = train_test_split(X_train, y_train, train_size=2780918, \n",
    "                                  random_state=31)\n",
    "\n",
    "drandom = prepare_DMatrix(X_t2, y_t2)\n",
    "bst2 = xgb.train(params, drandom, num_boost_round=2000, evals=[(dvalid, 'validation')], # evalation set for determining early stopping\n",
    "                early_stopping_rounds=100, verbose_eval=False)\n",
    "\n",
    "\n",
    "y_pred_proba_2 = bst2.predict(dtest)\n",
    "\n",
    "roc_auc_random = roc_auc_score(y_test, y_pred_proba_2)\n",
    "test_scores2.append(roc_auc_random)\n",
    "print(f\"ROC AUC Random Score: {roc_auc_random}\")\n",
    "\n",
    "del bst2, X_t2, y_t2, drandom, dtest, dvalid\n",
    "gc.collect()\n",
    "# check if adding tree_method='gpu_hist', predictor='gpu_predictor' increase the ROC_AUC_Score\n",
    "# By default, XGBClassifier uses a threshold of 0.5 for classification.\n",
    "# We can change the threshold after training to improve metrics such as precision, recall, and F1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have about 8.5 million additional data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_rest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.3.2 Progressively add more samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import trange\n",
    "\n",
    "dvalid = prepare_DMatrix(X_valid, y_valid)\n",
    "dtest = prepare_DMatrix(X_test, y_test)\n",
    "\n",
    "for n in trange(1, 9):\n",
    "    start_time = time.time()  # Timing starts\n",
    "    \n",
    "    if n * 1000000 < len(X_rest):\n",
    "        X_t, _, y_t, _ = train_test_split(X_rest, y_rest, train_size=n * 1000000, random_state=31)\n",
    "        X_t = pd.concat([X_minimal, X_t], axis=0)\n",
    "        y_t = pd.concat([y_minimal, y_t], axis=0)\n",
    "        X_t2, _, y_t2, _ = train_test_split(X_train, y_train, train_size=n * 1000000 + 2780918, random_state=31)\n",
    "    else:\n",
    "        X_t, y_t = X_train, y_train\n",
    "        X_t2, y_t2 = X_train, y_train\n",
    "\n",
    "    print(f\"Preparing DMatrix for n={n}\")  # Logging preparation\n",
    "    dMatrix_temporary = prepare_DMatrix(X_t, y_t)\n",
    "    dMatrix_temporary_2 = prepare_DMatrix(X_t2, y_t2)\n",
    "    # Train new instances of model to ensure Isolation of experiment\n",
    "    clf = xgb.train(params, dMatrix_temporary, num_boost_round=2000, evals=[(dvalid, 'validation')],\n",
    "                    early_stopping_rounds=100, verbose_eval=False)\n",
    "    clf2 = xgb.train(params, dMatrix_temporary_2, num_boost_round=2000, evals=[(dvalid, 'validation')], \n",
    "                     early_stopping_rounds=100, verbose_eval=False)\n",
    "\n",
    "    test_scores.append(roc_auc_score(y_test, clf.predict(dtest)))\n",
    "    test_scores2.append(roc_auc_score(y_test, clf2.predict(dtest)))\n",
    "\n",
    "    del clf, clf2, dMatrix_temporary, dMatrix_temporary_2\n",
    "    gc.collect()\n",
    "    \n",
    "    end_time = time.time()  # Timing ends\n",
    "    print(f\"Iteration {n} completed in {end_time - start_time:.2f} seconds\")  # Logging time taken\n",
    "\n",
    "del dtest, dvalid\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Save the score in file using pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save test_scores\n",
    "with open('test_scores.pkl', 'wb') as f:\n",
    "    pickle.dump(test_scores, f)\n",
    "\n",
    "# Save test_scores2\n",
    "with open('test_scores2.pkl', 'wb') as f:\n",
    "    pickle.dump(test_scores2,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4,1 load test scores from pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load test_scores\n",
    "with open('test_scores.pkl', 'rb') as f:\n",
    "    test_scores = pickle.load(f)\n",
    "\n",
    "# Load test_scores2\n",
    "with open('test_scores2.pkl', 'rb') as f:\n",
    "    test_scores2 =pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "n = [0,1,2,3,4,5,6,7,8]\n",
    "plt.plot(n, test_scores2, marker='o', linestyle='-', color='r', label='random subsample')\n",
    "plt.plot(n, test_scores, marker='o', linestyle='-', color='b',  label='downsampling')\n",
    "plt.xlabel('Number of samples (x10^6)')\n",
    "plt.ylabel('Test Auc Scores')\n",
    "plt.title('Performance Gain of Adding data rows with Response = 0')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 XGBoost Base Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc',\n",
    "    'max_bin': 262143,\n",
    "    'tree_method': 'hist',\n",
    "    'device' : 'cuda', \n",
    "    'seed' : 42\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the Max_bin to be unusually high, becuause in an unbalanced dataset, features of the majority class often overshadow those minority class ones.\n",
    "\n",
    "Instead of grouping all values of a feature into a few broad bins, the model can create many narrow bins, capturing subtle variations in the data.\n",
    "\n",
    "For example, if a feature slightly differs between the minority and majority classes, a higher max_bin value can help the model recognize these differences more accurately, which might not be possible with fewer bins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 XGBoost model evaluation function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "\n",
    "def evaluate_model_performance_xgboost(model, dvalid, y_valid, threshold=0.5):\n",
    "    # Predict probabilities for the validation set\n",
    "    y_valid_pred_proba = model.predict(dvalid)\n",
    "    y_valid_pred = (y_valid_pred_proba >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate the confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_valid, y_valid_pred).ravel()\n",
    "    \n",
    "    # Calculate True Positive Rate (TPR) and True Negative Rate (TNR)\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (fp + tn)\n",
    "    precision = tp / (tp + fp)\n",
    "    negative_precision = tn / (tn + fn) #the proportion of true negatives among all predicted negatives\n",
    "\n",
    "    # Calculate AUC-ROC score\n",
    "    roc_auc = roc_auc_score(y_valid, y_valid_pred_proba)\n",
    "    \n",
    "    # Print the evaluation metrics\n",
    "    print('Sensitivity (TPR):', sensitivity)\n",
    "    print('Specificity (TNR):', specificity)\n",
    "    print('Precision:', precision)\n",
    "    print('Negative Precision:', negative_precision)\n",
    "    print('AUC-ROC Score:', roc_auc)\n",
    "\n",
    "    return sensitivity, specificity, precision, negative_precision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 XGBoost, downsampled  balanced dataset, with base params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score,roc_curve\n",
    "import gc\n",
    "\n",
    "\n",
    "dvalid = prepare_DMatrix(X_valid, y_valid)\n",
    "# dvalid_selected = prepare_DMatrix(X_valid_selected, y_valid)\n",
    "dminimal = prepare_DMatrix(X_minimal, y_minimal)\n",
    "# dminimal_selected = prepare_DMatrix(X_minimal_5_selected, y_minimal)\n",
    "\n",
    "# Training with early stopping\n",
    "bst = xgb.train(params, dminimal, num_boost_round=2000, evals=[(dvalid, 'validation')],\n",
    "                early_stopping_rounds=100, verbose_eval=False)\n",
    "\n",
    "evaluate_model_performance_xgboost(bst, dvalid, y_valid, threshold = 0.5)\n",
    "\n",
    "# Clean up memory\n",
    "del bst,dvalid,dminimal,\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 XGBoost, downsampled , balanced + 3000000 (label = 0) dataset, with base params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "# Split and combine the data\n",
    "X_3mil, _, y_3mil, _ = train_test_split(X_rest, y_rest, train_size=3 * 1000000, random_state=31, stratify=y_rest)\n",
    "\n",
    "# Combine minimal dataset with additional samples\n",
    "X_concat_3mil = pd.concat([X_minimal, X_3mil], axis=0)\n",
    "y_concat_3mil = pd.concat([y_minimal, y_3mil], axis=0)\n",
    "\n",
    "\n",
    "dvalid = prepare_DMatrix(X_valid, y_valid)\n",
    "# dvalid_selected = prepare_DMatrix(X_valid_selected, y_valid)\n",
    "d_concat_3mil = prepare_DMatrix(X_concat_3mil, y_concat_3mil)\n",
    "\n",
    "# Training with early stopping\n",
    "bst2 = xgb.train(params, d_concat_3mil, num_boost_round=2000, evals=[(dvalid, 'validation')],\n",
    "                early_stopping_rounds=100, verbose_eval=False)\n",
    "\n",
    "evaluate_model_performance_xgboost(bst2, dvalid, y_valid, threshold = 0.5)\n",
    "del dvalid,d_concat_3mil\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Feature Importance (gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst2.feature_names = feature_names\n",
    "xgb.plot_importance(bst2, importance_type='gain')\n",
    "plt.title('Feature Importance (Gain)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**gain**: gain in AUC_ROC score\n",
    "\n",
    "In Feature of importance (gain), the top 2 features are **\"Previously_Insured\"** and **\"Vehicle_Damage\"**. These features have the highest importance scores, indicating they contribute the most to the model's predictive power in terms of gain.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Feature Importance (weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(bst2, importance_type='weight')\n",
    "plt.title('Feature Importance (Weight)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**weight**: most commonly used to separate nodes, not directly related to increase in AUC_ROC score\n",
    "\n",
    "The top features by weight are **\"Annual_Premium\"** and **\"Vintage\"**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Feature Engineering -- Removal of Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Baseline AUC-ROC score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume df is your DataFrame and it contains the target and features\n",
    "# Replace 'target' with your actual target column name\n",
    "feature_names = [\n",
    "    'Gender','Age','Driving_License','Region_Code','Previously_Insured','Vehicle_Age','Vehicle_Damage','Annual_Premium','Policy_Sales_Channel','Vintage']\n",
    "# Split and combine the data\n",
    "X_3mil, _, y_3mil, _ = train_test_split(X_rest, y_rest, train_size=3 * 1000000, random_state=31, stratify=y_rest)\n",
    "\n",
    "# Combine minimal dataset with additional samples\n",
    "X_concat_3mil = pd.concat([X_minimal, X_3mil], axis=0)\n",
    "y_concat_3mil = pd.concat([y_minimal, y_3mil], axis=0)\n",
    "\n",
    "\n",
    "dvalid = prepare_DMatrix(X_valid, y_valid)\n",
    "# dvalid_selected = prepare_DMatrix(X_valid_selected, y_valid)\n",
    "d_concat_3mil = prepare_DMatrix(X_concat_3mil, y_concat_3mil)\n",
    "\n",
    "# Dictionary to store the ROC AUC scores for each dropped predictor\n",
    "auc_scores = {}\n",
    "\n",
    "# Evaluate baseline model with all predictors\n",
    "bst_all = xgb.train(params, d_concat_3mil, num_boost_round=2000, evals=[(dvalid, 'validation')],\n",
    "                    early_stopping_rounds=100, verbose_eval=False)\n",
    "\n",
    "preds_all = bst_all.predict(dvalid)\n",
    "auc_all = roc_auc_score(y_valid, preds_all)\n",
    "auc_scores[\"without_dropping\"] = auc_all\n",
    "print(f'Baseline AUC with all features: {auc_all:.4f}')\n",
    "\n",
    "del dvalid,d_concat_3mil, bst_all\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 AUC score with 1 predictor removed per iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Iterate over each predictor and evaluate performance without it\n",
    "for predictor in feature_names:\n",
    "    # Create training data without the current predictor\n",
    "    reduced_features = [f for f in feature_names if f != predictor]\n",
    "    X_concat_3mil_reduced = X_concat_3mil[reduced_features]\n",
    "    X_valid_reduced = X_valid[reduced_features]\n",
    "\n",
    "    # Prepare DMatrices\n",
    "    d_concat_3mil_reduced = prepare_DMatrix(X_concat_3mil_reduced, y_concat_3mil)\n",
    "    dvalid_reduced = prepare_DMatrix(X_valid_reduced, y_valid)\n",
    "\n",
    "    # Train the model\n",
    "    bst_reduced = xgb.train(params, d_concat_3mil_reduced, num_boost_round=2000, evals=[(dvalid_reduced, 'validation')],\n",
    "                            early_stopping_rounds=100, verbose_eval=False)\n",
    "\n",
    "    # Predict and evaluate\n",
    "    preds_reduced = bst_reduced.predict(dvalid_reduced)\n",
    "    auc_reduced = roc_auc_score(y_valid, preds_reduced)\n",
    "\n",
    "    auc_scores[predictor] = auc_reduced\n",
    "    print(f'AUC without {predictor}: {auc_reduced:.4f}')\n",
    "\n",
    "    del dvalid_reduced ,d_concat_3mil_reduced, bst_reduced\n",
    "    gc.collect()\n",
    "# Find the predictor with the highest AUC score\n",
    "best_predictor = max(auc_scores, key=auc_scores.get)\n",
    "highest_auc_score = auc_scores[best_predictor]\n",
    "\n",
    "# Print the best predictor and its AUC score\n",
    "print(f\"Highest AUC score with predictor removed: {best_predictor}: {highest_auc_score:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC without **\"Annual_Premium\"** and **\"Vintage\"** decreases the most, suggesting these are the most important predictors for the target variables.\n",
    "\n",
    "AUC without **\"Driving_License\"** increases the most, possibly indicating that this feature might be less informative or noisy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 AUC score with \"Driving_License\"+ 1 predictor removed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline model, only drops the \"Driving License\"\n",
    "feature_names_without_Driving_License = [\n",
    "    'Gender','Age','Region_Code','Previously_Insured','Vehicle_Age','Vehicle_Damage','Annual_Premium','Policy_Sales_Channel','Vintage']\n",
    "# Define the model parameters\n",
    "params = {'objective': 'binary:logistic', 'eval_metric': 'auc','max_bin': 262143,'tree_method': 'hist','device' : 'cuda','seed' : 42}\n",
    "# Dictionary to store the ROC AUC scores for each dropped predictor\n",
    "auc_scores_remove_2_predictors = {}\n",
    "\n",
    "# Split and combine the data\n",
    "X_3mil, _, y_3mil, _ = train_test_split(X_rest, y_rest, train_size=3 * 1000000, random_state=31, stratify=y_rest)\n",
    "\n",
    "# Combine minimal dataset with additional samples\n",
    "X_concat_3mil = pd.concat([X_minimal, X_3mil], axis=0)\n",
    "y_concat_3mil = pd.concat([y_minimal, y_3mil], axis=0)\n",
    "\n",
    "X_concat_3mil_no_Driving_License = X_concat_3mil.drop(\"Driving_License\", axis=1)\n",
    "X_valid_no_Driving_License = X_valid.drop(\"Driving_License\", axis=1)\n",
    "\n",
    "dvalid_no_Driving_License = prepare_DMatrix(X_valid_no_Driving_License , y_valid)\n",
    "# dvalid_selected = prepare_DMatrix(X_valid_selected, y_valid)\n",
    "d_concat_3mil_no_Driving_License = prepare_DMatrix(X_concat_3mil_no_Driving_License, y_concat_3mil)\n",
    "\n",
    "bst_all = xgb.train(params, d_concat_3mil_no_Driving_License, num_boost_round=2000, evals=[(dvalid_no_Driving_License , 'validation')],\n",
    "                    early_stopping_rounds=100, verbose_eval=False)\n",
    "\n",
    "preds_all = bst_all.predict(dvalid_no_Driving_License )\n",
    "auc_all = roc_auc_score(y_valid, preds_all)\n",
    "auc_scores_remove_2_predictors[\"baseline_drop_Driving_License\"] = auc_all\n",
    "print(f'Baseline AUC , only drops the \"Driving License\": {auc_all:.4f}')\n",
    "\n",
    "del dvalid_no_Driving_License ,d_concat_3mil_no_Driving_License, bst_all\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# Iterate over each predictor and evaluate performance without it\n",
    "for predictor in feature_names_without_Driving_License:\n",
    "    # Create training data without the current predictor\n",
    "    reduced_features = [f for f in feature_names_without_Driving_License if f != predictor]\n",
    "    X_concat_3mil_reduced = X_concat_3mil[reduced_features]\n",
    "    X_valid_reduced = X_valid[reduced_features]\n",
    "\n",
    "    # Prepare DMatrices\n",
    "    d_concat_3mil_reduced = prepare_DMatrix(X_concat_3mil_reduced, y_concat_3mil)\n",
    "    dvalid_reduced = prepare_DMatrix(X_valid_reduced, y_valid)\n",
    "\n",
    "    # Train the model\n",
    "    bst_reduced = xgb.train(params, d_concat_3mil_reduced, num_boost_round=2000, evals=[(dvalid_reduced, 'validation')],\n",
    "                            early_stopping_rounds=100, verbose_eval=False)\n",
    "\n",
    "    # Predict and evaluate\n",
    "    preds_reduced = bst_reduced.predict(dvalid_reduced)\n",
    "    auc_reduced = roc_auc_score(y_valid, preds_reduced)\n",
    "\n",
    "    auc_scores_remove_2_predictors[predictor] = auc_reduced\n",
    "    print(f'AUC without \"Driving_License\" + {predictor}: {auc_reduced:.4f}')\n",
    "\n",
    "    del dvalid_reduced ,d_concat_3mil_reduced, bst_reduced\n",
    "    gc.collect()\n",
    "# Find the predictor with the highest AUC score\n",
    "best_predictor = max(auc_scores_remove_2_predictors, key=auc_scores_remove_2_predictors.get)\n",
    "highest_auc_score = auc_scores_remove_2_predictors[best_predictor]\n",
    "\n",
    "# Print the best predictor and its AUC score\n",
    "print(f\"Highest AUC score with 'Driving_License' + predictor removed: {best_predictor}: {highest_auc_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC without **\"Driving_License\"** remains the highest among all combinations of removal with \"Driving_License\" included. This indicates that we won't need to further remove features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Feature Engineering -- Combining Features to increase predictive power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.1 We start with combining the most predictive feature — **\"Previously_Insured\"**, with one of the other predictors in each iteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "\n",
    "# Example data preparation\n",
    "# Assuming X_minimal, X_rest, y_minimal, y_rest, X_valid, and y_valid are already defined\n",
    "# Dictionary to store AUC scores\n",
    "auc_scores = {}\n",
    "\n",
    "# Define the model parameters\n",
    "\n",
    "# Combine datasets\n",
    "X_3mil, _, y_3mil, _ = train_test_split(X_rest, y_rest, train_size=3 * 1000000, random_state=31, stratify=y_rest)\n",
    "X_concat_3mil = pd.concat([X_minimal, X_3mil], axis=0)\n",
    "y_concat_3mil = pd.concat([y_minimal, y_3mil], axis=0)\n",
    "\n",
    "# List of feature names excluding 'Driving_License'\n",
    "feature_names_without_Driving_License = [\n",
    "    'Gender', 'Age', 'Region_Code', 'Previously_Insured', \n",
    "    'Vehicle_Age', 'Vehicle_Damage', 'Annual_Premium', \n",
    "    'Policy_Sales_Channel', 'Vintage'\n",
    "]\n",
    "\n",
    "# Ensure 'Driving_License' is not in the data\n",
    "X_concat_3mil_no_Driving_License = X_concat_3mil.drop(\"Driving_License\", axis=1)\n",
    "X_valid_no_Driving_License = X_valid.drop(\"Driving_License\", axis=1)\n",
    "\n",
    "\n",
    "feature_names_no_Prev = [\n",
    "    'Gender', 'Age', 'Region_Code', \n",
    "    'Vehicle_Age', 'Vehicle_Damage', 'Annual_Premium', \n",
    "    'Policy_Sales_Channel', 'Vintage'\n",
    "]\n",
    "\n",
    "# Iterate over each feature and evaluate performance with Previously_Insured\n",
    "for feature in feature_names_no_Prev:\n",
    "    # Create a new combined feature\n",
    "    combined_feature_name = f'Previously_Insured_{feature}'\n",
    "\n",
    "    # avoid changing the originial datasets\n",
    "    df1 = X_concat_3mil_no_Driving_License.copy()\n",
    "    df_valid1 = X_valid_no_Driving_License.copy()\n",
    "\n",
    "# Create the new combined feature\n",
    "    df1[combined_feature_name] = pd.factorize(df1['Previously_Insured'].astype(str) + '_' + df1[feature].astype(str))[0] # 8 features + 1 combined feature\n",
    "    df_valid1[combined_feature_name] = pd.factorize(df_valid1['Previously_Insured'].astype(str) + '_' + df_valid1[feature].astype(str))[0] #  pd.factorize(array) returns a tuple of (codes, uniques). Since we are just interested in codes, we use [0]\n",
    "\n",
    "    # Prepare DMatrices\n",
    "    d_combined_3mil = prepare_DMatrix(df1, y_concat_3mil)\n",
    "    dvalid_combined =  prepare_DMatrix(df_valid1, y_valid)\n",
    "\n",
    "    # Train the model\n",
    "    bst_combined = xgb.train(params, d_combined_3mil, num_boost_round=2000,\n",
    "                             evals=[(dvalid_combined, 'validation')],\n",
    "                             early_stopping_rounds=100, verbose_eval=False)\n",
    "\n",
    "    # Predict and evaluate\n",
    "    preds_combined = bst_combined.predict(dvalid_combined)\n",
    "    auc_combined = roc_auc_score(y_valid, preds_combined)\n",
    "\n",
    "    auc_scores[feature] = auc_combined\n",
    "    print(f'AUC score combining Previously_Insured and {feature}: {auc_combined:.4f}')\n",
    "\n",
    "    # Clean up to free memory\n",
    "    del dvalid_combined, d_combined_3mil, bst_combined\n",
    "    gc.collect()\n",
    "\n",
    "# Find the feature with the highest AUC score when combined with Previously_Insured\n",
    "best_feature = max(auc_scores, key=auc_scores.get)\n",
    "highest_auc_score = auc_scores[best_feature]\n",
    "\n",
    "# Print the best feature and its AUC score\n",
    "print(f\"Highest AUC score combining 'Previously_Insured' and {best_feature}: {highest_auc_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Highest AUC score is from the baseline model, without combining features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.2 The second approach is based on the correlation matrix. We combine two less correlated features, which may provide complementary information when combined, hence increasing the AUC-ROC score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Fine tuning of XGBoost model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1 learning rate (default eta = 0.3 is the best )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2  scale_pos_weight [default=1] \n",
    "\n",
    "The `scale_pos_weight` parameter in XGBoost is specifically applied to the positive class. In the context of an imbalanced dataset (positive: 1, negative: 10), adjusting this parameter can improve the model's sensitivity to the positive class.\n",
    "\n",
    "During training, 60% of the data is used from X_rest. The typical value for `scale_pos_weight` is approximately 4.678, calculated as the ratio of negative to positive instances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Suggest a value for scale_pos_weight\n",
    "    scale_pos_weight = trial.suggest_float('scale_pos_weight', 1, 1.20)\n",
    "\n",
    "    # Update the parameters with the suggested scale_pos_weight\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'max_bin': 262143,\n",
    "        'tree_method': 'hist',\n",
    "        'device': 'cuda',\n",
    "        'min_child_weight' : 2.5,\n",
    "        'scale_pos_weight': scale_pos_weight\n",
    "    }\n",
    "    X_rest_0_6, _, y_rest_0_6, _ = train_test_split(X_rest, y_rest, train_size=0.6, random_state=31, stratify=y_rest)\n",
    "\n",
    "    X_imbalanced = pd.concat([X_minimal, X_rest_0_6], axis=0)\n",
    "    y_imbalanced = pd.concat([y_minimal, y_rest_0_6], axis=0)\n",
    "    \n",
    "    X_imbalanced = X_imbalanced.drop(\"Driving_License\", axis=1)\n",
    "    X_valid_imbalanced = X_valid.drop(\"Driving_License\", axis=1)\n",
    "\n",
    "    d_imbalanced = prepare_DMatrix(X_imbalanced, y_imbalanced)\n",
    "    d_val = prepare_DMatrix(X_valid_imbalanced, y_valid)\n",
    "\n",
    "    # Train the model\n",
    "    model = xgb.train(params, d_imbalanced, num_boost_round=2000, evals=[(d_val, 'validation')],\n",
    "                      early_stopping_rounds=100, verbose_eval=False)\n",
    "\n",
    "    # Predict and evaluate the model\n",
    "    y_pred = model.predict(d_val)\n",
    "    score = roc_auc_score(y_valid, y_pred)\n",
    "\n",
    "    del d_imbalanced,d_val,model\n",
    "    gc.collect()\n",
    "    return score\n",
    "\n",
    "# Set up Optuna study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Get the best trial and its parameters\n",
    "best_trial = study.best_trial\n",
    "best_scale_pos_weight = best_trial.params['scale_pos_weight']\n",
    "best_score = best_trial.value\n",
    "\n",
    "print(f\"Best scale_pos_weight: {best_scale_pos_weight} with AUC-ROC: {best_score:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Max_depth (default=6 is the best)  \n",
    "\n",
    "Maximum depth of tree, where increasing it would increase the risk of overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "def find_best_max_depth(base_params, max_depth_values):\n",
    "    best_max_depth = None\n",
    "    best_score = -np.inf\n",
    "    best_oof_preds = None\n",
    "    max_depth_auc_roc_scores = {}\n",
    "\n",
    "    for max_depth in max_depth_values:\n",
    "        print(f\"Testing max_depth={max_depth}\")\n",
    "        params = base_params.copy()\n",
    "        params['max_depth'] = max_depth\n",
    "\n",
    "        # Down-sample the majority class\n",
    "        X_rest_0_6, _, y_rest_0_6, _ = train_test_split(X_rest, y_rest, train_size=0.6, random_state=31, stratify=y_rest)\n",
    "\n",
    "        X_imbalanced = pd.concat([X_minimal, X_rest_0_6], axis=0)\n",
    "        y_imbalanced = pd.concat([y_minimal, y_rest_0_6], axis=0)\n",
    "\n",
    "        X_imbalanced = X_imbalanced.drop(\"Driving_License\", axis=1)\n",
    "        X_valid_imbalanced = X_valid.drop(\"Driving_License\", axis=1)\n",
    "\n",
    "        d_imbalanced = prepare_DMatrix(X_imbalanced, y_imbalanced)\n",
    "        d_val = prepare_DMatrix(X_valid_imbalanced, y_valid)\n",
    "\n",
    "        # Train the model\n",
    "        model = xgb.train(params, d_imbalanced, num_boost_round=2000, evals=[(d_val, 'validation')],\n",
    "                          early_stopping_rounds=100, verbose_eval=False)\n",
    "\n",
    "        # Predict and evaluate the model\n",
    "        y_pred = model.predict(d_val)\n",
    "        score = roc_auc_score(y_valid, y_pred)\n",
    "        max_depth_auc_roc_scores[max_depth] = score\n",
    "        print(f\"ROC-AUC for max_depth={max_depth}: {score:.5f}\")\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_max_depth = max_depth\n",
    "            best_oof_preds = y_pred\n",
    "\n",
    "        del d_imbalanced, d_val, model\n",
    "        gc.collect()\n",
    "\n",
    "    print(f\"Best max_depth: {best_max_depth} with ROC-AUC: {best_score:.5f}\")\n",
    "    return best_max_depth, best_oof_preds, max_depth_auc_roc_scores\n",
    "\n",
    "# Plotting function\n",
    "def plot_max_depth_vs_roc_auc(max_depth_auc_roc_scores):\n",
    "    max_depth_values = list(max_depth_auc_roc_scores.keys())\n",
    "    roc_auc_scores = list(max_depth_auc_roc_scores.values())\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(max_depth_values, roc_auc_scores, marker='o', linestyle='-', color='b', label='ROC-AUC')\n",
    "    optimal_index = np.argmax(roc_auc_scores)\n",
    "    optimal_max_depth = max_depth_values[optimal_index]\n",
    "    optimal_auc = roc_auc_scores[optimal_index]\n",
    "    plt.scatter(optimal_max_depth, optimal_auc, color='r', label=f'Optimal max_depth: {optimal_max_depth}', zorder=5)\n",
    "    plt.title('max_depth vs. ROC-AUC')\n",
    "    plt.xlabel('max_depth')\n",
    "    plt.ylabel('ROC-AUC')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Example parameters\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc',\n",
    "    'max_bin': 262143,\n",
    "    'tree_method': 'hist',\n",
    "    'device': 'cuda'\n",
    "}\n",
    "\n",
    "# Define the range of max_depth values to test\n",
    "max_depth_values = range(3, 11)\n",
    "\n",
    "# Call the function\n",
    "best_max_depth, oof_preds, max_depth_auc_roc_scores = find_best_max_depth(params, max_depth_values)\n",
    "plot_max_depth_vs_roc_auc(max_depth_auc_roc_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Scale min_child_weight [default = 1]\n",
    "\n",
    "It specifies the minimum sum of instance weight (hessian) needed in a child node.\n",
    "\n",
    "It is a form of regularization to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "\n",
    "\n",
    "def find_best_min_child_weight(base_params, min_child_weight_values):\n",
    "    best_min_child_weight = None\n",
    "    best_score = -np.inf\n",
    "    best_oof_preds = None\n",
    "    best_test_preds = None\n",
    "    min_child_weight_auc_roc_scores = {}\n",
    "\n",
    "    for min_child_weight in min_child_weight_values:\n",
    "        print(f\"Testing min_child_weight={min_child_weight}\")\n",
    "        params = base_params.copy()\n",
    "        params['min_child_weight'] = min_child_weight\n",
    "\n",
    "        # Down-sample the majority class\n",
    "        X_rest_0_6, _, y_rest_0_6, _ = train_test_split(X_rest, y_rest, train_size=0.6, random_state=31, stratify=y_rest)\n",
    "\n",
    "        X_imbalanced = pd.concat([X_minimal, X_rest_0_6], axis=0)\n",
    "        y_imbalanced = pd.concat([y_minimal, y_rest_0_6], axis=0)\n",
    "\n",
    "        X_imbalanced = X_imbalanced.drop(\"Driving_License\", axis=1)\n",
    "        X_valid_imbalanced = X_valid.drop(\"Driving_License\", axis=1)\n",
    "\n",
    "        d_imbalanced = prepare_DMatrix(X_imbalanced, y_imbalanced)\n",
    "        d_val = prepare_DMatrix(X_valid_imbalanced, y_valid)\n",
    "\n",
    "        model = xgb.train(params, d_imbalanced, num_boost_round=2000, evals=[(d_val, 'validation')],\n",
    "                          early_stopping_rounds=100, verbose_eval=False)\n",
    "        y_pred = model.predict(d_val)\n",
    "        score = roc_auc_score(y_valid, y_pred)\n",
    "        min_child_weight_auc_roc_scores[min_child_weight] = score\n",
    "        print(f\"ROC-AUC for min_child_weight={min_child_weight}: {score:.5f}\")\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_min_child_weight = min_child_weight\n",
    "            best_oof_preds = y_pred\n",
    "\n",
    "        del d_imbalanced, d_val\n",
    "        gc.collect()\n",
    "\n",
    "    print(f\"Best min_child_weight: {best_min_child_weight} with ROC-AUC: {best_score:.5f}\")\n",
    "\n",
    "    # Base parameters for XGBoost model\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc',\n",
    "    'max_bin': 262143,\n",
    "    'tree_method': 'hist',\n",
    "    'device' : 'cuda'\n",
    "}\n",
    "\n",
    "# Define the range of min_child_weight values to test\n",
    "min_child_weight_values = [0.3,0.5, 1,1.5,2, 2.5]\n",
    "# 3.5 ,4, 4.5,5\n",
    "# Call the function to find the best min_child_weight\n",
    "find_best_min_child_weight(params, min_child_weight_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cross_validation + bagging \n",
    "\n",
    "--> XGBoost model only\n",
    "\n",
    "--> remove feature \"Driving_License\"\n",
    "\n",
    "--> with newly tuned hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc',\n",
    "    'max_bin': 262143,\n",
    "    'tree_method': 'hist',\n",
    "    'device' : 'cuda',\n",
    "    # 'eta' : 0.3 (default)\n",
    "    # 'max_depth' : 6 (default)\n",
    "    'scale_pos_weight' : 1.1705442976059786,\n",
    "    # 'min_child_weight' : 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import gc\n",
    "# for binary classification, use predict() instead of predict_proba\n",
    "\n",
    "def cross_validate(X ,y, test_data, n_splits = 10, n_bags = 3 ):\n",
    "\n",
    "    start_time = datetime.datetime.now()\n",
    "    scores = []\n",
    "    oof_preds = np.zeros(len(y))\n",
    "    test_preds = np.zeros(len(test_data))\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=n_splits, shuffle = True, random_state=63)\n",
    "\n",
    "    d_test_csv = prepare_DMatrix(test_data, np.zeros(test_data.shape[0])) # generate a DMatrix of test_data without labels = 0, does not affect prediction\n",
    "\n",
    "    for fold, (train_index, valid_index) in enumerate(kfold.split(X,y.astype(str))):\n",
    "        for i in range(0,n_bags):                                        \n",
    "            X_train = X.iloc[train_index]\n",
    "            y_train = y.iloc[train_index]\n",
    "            X_val = X.iloc[valid_index]\n",
    "            y_val = y.iloc[valid_index]\n",
    "            # bagging is aggregating the results predicted by different sub-trees (in XGBoost)\n",
    "            # We train the model using different, balanced, sub-datasets, with a random state = '10*fold + i'\n",
    "            X_train, y_train, X_rest, y_rest = down_sampling(X_train, y_train, 10*fold + i)\n",
    "            # double random to ensure the model capture more diverse patterns\n",
    "            X_rest_0_6, _, y_rest_0_6, _ = train_test_split(X_rest, y_rest, train_size=0.6, random_state=10*fold + i, stratify=y_rest)  #Increase train size to 0.9\n",
    "            # add 60% of label = 0 data into training \n",
    "            X_imbalanced = pd.concat([X_train, X_rest_0_6], axis=0)\n",
    "            y_imbalanced = pd.concat([y_train, y_rest_0_6], axis=0)\n",
    "            \n",
    "\n",
    "            d_imbalanced = prepare_DMatrix(X_imbalanced, y_imbalanced)\n",
    "            d_val = prepare_DMatrix(X_val, y_val)\n",
    "\n",
    "            m = xgb.train(params, d_imbalanced, num_boost_round=2000, evals=[(d_val, 'validation')],\n",
    "                early_stopping_rounds=100, verbose_eval=False)\n",
    "            y_pred = m.predict(d_val) # returns a 1D array of probability for Response == 1\n",
    "            # Measures the ability of the model to discriminate between positive and negative classes based on predicted probabilities\n",
    "            # Useful in imbalanced dataset\n",
    "            score = roc_auc_score(y_val,  y_pred)\n",
    "            print(f\"# Fold {fold}, bag {i}: ROC-AUC-Score={score:.5f}\")\n",
    "            scores.append(score)\n",
    "            # in stratified k-fold, every sample will ultimately be used as the validation set\n",
    "            # And for every index in the validation set, our overall prediction is based on averaging the predictions of 3 subtrees (n_bags = 3)\n",
    "            # So we divide the predictions of each subtree by 3(n_bags = 3), and sum the predictions\n",
    "            oof_preds[valid_index] += y_pred/n_bags\n",
    "            # calculate the test predictions base on the subtree created in current fold\n",
    "            # overall test prediction = average of 5 folds(n_splits = 5), each fold has 3 subtrees(n_bags = 3)\n",
    "            # So we divide the predictions of each subtree by 3(n_bags = 3),and further duvude it by the number of folds (n_splits = 5), and finally sum the predictions\n",
    "            \n",
    "            test_preds = test_preds + m.predict(d_test_csv)/kfold.get_n_splits()/n_bags\n",
    "            \n",
    "            del d_imbalanced, d_val\n",
    "            gc.collect()\n",
    "\n",
    "    del d_test_csv\n",
    "    gc.collect()\n",
    "    \n",
    "    elapsed_time = datetime.datetime.now() - start_time\n",
    "    print(f\"#ROC-AUC mean: {np.array(scores).mean():.7f} (+- {np.array(scores).std():.7f})\"\n",
    "              f\"#   elapsed time:   {int(np.round(elapsed_time.total_seconds() / 60))} min\")\n",
    "\n",
    "    return oof_preds, test_preds\n",
    "# but because of the huge class imbalance, in each fold the majority class is way more than the minority class, \n",
    "# so even though we use bagging with different random states, to choose different samples from the majority class, \n",
    "# there will still be some unused majority class sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Fold 0, bag 0: ROC-AUC-Score=0.89016\n",
      "# Fold 0, bag 1: ROC-AUC-Score=0.89010\n",
      "# Fold 0, bag 2: ROC-AUC-Score=0.89022\n",
      "# Fold 1, bag 0: ROC-AUC-Score=0.89050\n",
      "# Fold 1, bag 1: ROC-AUC-Score=0.89057\n",
      "# Fold 1, bag 2: ROC-AUC-Score=0.89054\n",
      "# Fold 2, bag 0: ROC-AUC-Score=0.89070\n",
      "# Fold 2, bag 1: ROC-AUC-Score=0.89082\n",
      "# Fold 2, bag 2: ROC-AUC-Score=0.89077\n",
      "# Fold 3, bag 0: ROC-AUC-Score=0.89069\n",
      "# Fold 3, bag 1: ROC-AUC-Score=0.89066\n",
      "# Fold 3, bag 2: ROC-AUC-Score=0.89085\n",
      "# Fold 4, bag 0: ROC-AUC-Score=0.89135\n",
      "# Fold 4, bag 1: ROC-AUC-Score=0.89133\n",
      "# Fold 4, bag 2: ROC-AUC-Score=0.89125\n",
      "# Fold 5, bag 0: ROC-AUC-Score=0.89134\n",
      "# Fold 5, bag 1: ROC-AUC-Score=0.89132\n",
      "# Fold 5, bag 2: ROC-AUC-Score=0.89138\n",
      "# Fold 6, bag 0: ROC-AUC-Score=0.89036\n",
      "# Fold 6, bag 1: ROC-AUC-Score=0.89055\n",
      "# Fold 6, bag 2: ROC-AUC-Score=0.89046\n",
      "# Fold 7, bag 0: ROC-AUC-Score=0.89056\n",
      "# Fold 7, bag 1: ROC-AUC-Score=0.89065\n",
      "# Fold 7, bag 2: ROC-AUC-Score=0.89069\n",
      "# Fold 8, bag 0: ROC-AUC-Score=0.89072\n",
      "# Fold 8, bag 1: ROC-AUC-Score=0.89065\n",
      "# Fold 8, bag 2: ROC-AUC-Score=0.89069\n",
      "# Fold 9, bag 0: ROC-AUC-Score=0.89076\n",
      "# Fold 9, bag 1: ROC-AUC-Score=0.89080\n",
      "# Fold 9, bag 2: ROC-AUC-Score=0.89075\n",
      "#ROC-AUC mean: 0.8907402 (+- 0.0003450)#   elapsed time:   99 min\n",
      "Total ROC-AUC: 0.8917940759292897\n"
     ]
    }
   ],
   "source": [
    "X = preprocess(df)\n",
    "y = X.pop(\"Response\")\n",
    "X_drop_driving_license = X.drop(\"Driving_License\", axis=1)\n",
    "# print(X_drop_driving_license.head())\n",
    "\n",
    "test_data = preprocess(df_test)\n",
    "test_data_drop_driving_license = test_data.drop(\"Driving_License\", axis=1)\n",
    "# print(test_data_drop_driving_license.head())\n",
    "\n",
    "xgb_oof, xgb_test = cross_validate(X_drop_driving_license, y,test_data_drop_driving_license, n_splits = 10, n_bags = 3 )\n",
    "print(f\"Total ROC-AUC: {roc_auc_score(y, xgb_oof)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id  Response\n",
      "0  11504798  0.006490\n",
      "1  11504799  0.753227\n",
      "2  11504800  0.351327\n",
      "3  11504801  0.000147\n",
      "4  11504802  0.280768\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'test' is your test DataFrame and 'xgb_test' contains the predicted probabilities\n",
    "# Ensure 'test' contains the 'id' column with the corresponding IDs\n",
    "df_test_with_id = pd.read_csv(\"test.csv\")\n",
    "# Create the submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'id': df_test_with_id ['id'],  # Replace 'id' with the appropriate identifier column name\n",
    "    'Response': xgb_test  # The predicted probabilities\n",
    "})\n",
    "\n",
    "# Save the submission DataFrame to a CSV file\n",
    "submission.to_csv('submission_03.csv', index=False)\n",
    "\n",
    "# Display the first few rows of the submission DataFrame (optional)\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insight: When we use more than 1 downsampling/splitting steps in bagging, we should use different random_state for ***EVERY*** steps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
